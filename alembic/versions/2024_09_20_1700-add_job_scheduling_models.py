"""Add job scheduling models

Revision ID: add_job_scheduling_models
Revises: add_travel_advisory_scraping_models
Create Date: 2024-09-20 17:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = 'add_job_scheduling_models'
down_revision = 'add_travel_advisory_scraping_models'
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Create ENUM types
    op.execute("CREATE TYPE jobstatus AS ENUM ('PENDING', 'STARTED', 'RETRY', 'FAILURE', 'SUCCESS', 'REVOKED', 'DEAD_LETTER')")
    op.execute("CREATE TYPE jobpriority AS ENUM ('LOW', 'NORMAL', 'HIGH', 'CRITICAL')")

    # Create scraping_jobs table
    op.create_table('scraping_jobs',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('celery_task_id', sa.String(length=255), nullable=False),
        sa.Column('task_name', sa.String(length=255), nullable=False),
        sa.Column('source', sa.String(length=100), nullable=True),
        sa.Column('country', sa.String(length=100), nullable=True),
        sa.Column('priority', postgresql.ENUM('LOW', 'NORMAL', 'HIGH', 'CRITICAL', name='jobpriority'), nullable=True),
        sa.Column('queue_name', sa.String(length=100), nullable=False),
        sa.Column('scheduled_at', sa.DateTime(), nullable=True),
        sa.Column('started_at', sa.DateTime(), nullable=True),
        sa.Column('completed_at', sa.DateTime(), nullable=True),
        sa.Column('status', postgresql.ENUM('PENDING', 'STARTED', 'RETRY', 'FAILURE', 'SUCCESS', 'REVOKED', 'DEAD_LETTER', name='jobstatus'), nullable=True),
        sa.Column('retry_count', sa.Integer(), nullable=True),
        sa.Column('max_retries', sa.Integer(), nullable=True),
        sa.Column('result', sa.JSON(), nullable=True),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.Column('traceback', sa.Text(), nullable=True),
        sa.Column('duration_seconds', sa.Float(), nullable=True),
        sa.Column('memory_usage_mb', sa.Float(), nullable=True),
        sa.Column('job_config', sa.JSON(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('celery_task_id')
    )

    # Create indexes for scraping_jobs
    op.create_index('idx_scraping_jobs_celery_task_id', 'scraping_jobs', ['celery_task_id'], unique=False)
    op.create_index('idx_scraping_jobs_created_at', 'scraping_jobs', ['created_at'], unique=False)
    op.create_index('idx_scraping_jobs_priority', 'scraping_jobs', ['priority'], unique=False)
    op.create_index('idx_scraping_jobs_scheduled_at', 'scraping_jobs', ['scheduled_at'], unique=False)
    op.create_index('idx_scraping_jobs_source', 'scraping_jobs', ['source'], unique=False)
    op.create_index('idx_scraping_jobs_status', 'scraping_jobs', ['status'], unique=False)
    op.create_index('idx_scraping_jobs_task_name', 'scraping_jobs', ['task_name'], unique=False)

    # Create job_metrics table
    op.create_table('job_metrics',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('period_start', sa.DateTime(), nullable=False),
        sa.Column('period_end', sa.DateTime(), nullable=False),
        sa.Column('source', sa.String(length=100), nullable=True),
        sa.Column('total_jobs', sa.Integer(), nullable=True),
        sa.Column('successful_jobs', sa.Integer(), nullable=True),
        sa.Column('failed_jobs', sa.Integer(), nullable=True),
        sa.Column('retried_jobs', sa.Integer(), nullable=True),
        sa.Column('dead_letter_jobs', sa.Integer(), nullable=True),
        sa.Column('avg_duration_seconds', sa.Float(), nullable=True),
        sa.Column('min_duration_seconds', sa.Float(), nullable=True),
        sa.Column('max_duration_seconds', sa.Float(), nullable=True),
        sa.Column('avg_memory_usage_mb', sa.Float(), nullable=True),
        sa.Column('success_rate', sa.Float(), nullable=True),
        sa.Column('new_advisories_count', sa.Integer(), nullable=True),
        sa.Column('updated_advisories_count', sa.Integer(), nullable=True),
        sa.Column('metadata', sa.JSON(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )

    # Create indexes for job_metrics
    op.create_index('idx_job_metrics_created_at', 'job_metrics', ['created_at'], unique=False)
    op.create_index('idx_job_metrics_period', 'job_metrics', ['period_start', 'period_end'], unique=False)
    op.create_index('idx_job_metrics_source', 'job_metrics', ['source'], unique=False)

    # Create dead_letter_jobs table
    op.create_table('dead_letter_jobs',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('original_job_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('celery_task_id', sa.String(length=255), nullable=False),
        sa.Column('task_name', sa.String(length=255), nullable=False),
        sa.Column('original_args', sa.JSON(), nullable=True),
        sa.Column('original_kwargs', sa.JSON(), nullable=True),
        sa.Column('failure_reason', sa.Text(), nullable=False),
        sa.Column('final_error_message', sa.Text(), nullable=True),
        sa.Column('final_traceback', sa.Text(), nullable=True),
        sa.Column('total_retry_attempts', sa.Integer(), nullable=True),
        sa.Column('processed', sa.Boolean(), nullable=True),
        sa.Column('requeued', sa.Boolean(), nullable=True),
        sa.Column('manual_intervention_required', sa.Boolean(), nullable=True),
        sa.Column('resolution_notes', sa.Text(), nullable=True),
        sa.Column('resolved_by', sa.String(length=100), nullable=True),
        sa.Column('resolved_at', sa.DateTime(), nullable=True),
        sa.Column('moved_to_dlq_at', sa.DateTime(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )

    # Create indexes for dead_letter_jobs
    op.create_index('idx_dead_letter_jobs_manual_intervention', 'dead_letter_jobs', ['manual_intervention_required'], unique=False)
    op.create_index('idx_dead_letter_jobs_moved_at', 'dead_letter_jobs', ['moved_to_dlq_at'], unique=False)
    op.create_index('idx_dead_letter_jobs_processed', 'dead_letter_jobs', ['processed'], unique=False)
    op.create_index('idx_dead_letter_jobs_task_name', 'dead_letter_jobs', ['task_name'], unique=False)

    # Create scheduler_health table
    op.create_table('scheduler_health',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('check_type', sa.String(length=100), nullable=False),
        sa.Column('status', sa.String(length=50), nullable=False),
        sa.Column('response_time_ms', sa.Float(), nullable=True),
        sa.Column('queue_length', sa.Integer(), nullable=True),
        sa.Column('active_workers', sa.Integer(), nullable=True),
        sa.Column('details', sa.JSON(), nullable=True),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.Column('checked_at', sa.DateTime(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )

    # Create indexes for scheduler_health
    op.create_index('idx_scheduler_health_check_type', 'scheduler_health', ['check_type'], unique=False)
    op.create_index('idx_scheduler_health_checked_at', 'scheduler_health', ['checked_at'], unique=False)
    op.create_index('idx_scheduler_health_status', 'scheduler_health', ['status'], unique=False)

    # Create rate_limit_tracker table
    op.create_table('rate_limit_tracker',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('source', sa.String(length=100), nullable=False),
        sa.Column('time_window_start', sa.DateTime(), nullable=False),
        sa.Column('time_window_end', sa.DateTime(), nullable=False),
        sa.Column('requests_made', sa.Integer(), nullable=True),
        sa.Column('requests_allowed', sa.Integer(), nullable=False),
        sa.Column('requests_blocked', sa.Integer(), nullable=True),
        sa.Column('avg_response_time_ms', sa.Float(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )

    # Create indexes for rate_limit_tracker
    op.create_index('idx_rate_limit_tracker_created_at', 'rate_limit_tracker', ['created_at'], unique=False)
    op.create_index('idx_rate_limit_tracker_source', 'rate_limit_tracker', ['source'], unique=False)
    op.create_index('idx_rate_limit_tracker_window', 'rate_limit_tracker', ['time_window_start', 'time_window_end'], unique=False)

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Drop indexes
    op.drop_index('idx_rate_limit_tracker_window', table_name='rate_limit_tracker')
    op.drop_index('idx_rate_limit_tracker_source', table_name='rate_limit_tracker')
    op.drop_index('idx_rate_limit_tracker_created_at', table_name='rate_limit_tracker')
    op.drop_table('rate_limit_tracker')

    op.drop_index('idx_scheduler_health_status', table_name='scheduler_health')
    op.drop_index('idx_scheduler_health_checked_at', table_name='scheduler_health')
    op.drop_index('idx_scheduler_health_check_type', table_name='scheduler_health')
    op.drop_table('scheduler_health')

    op.drop_index('idx_dead_letter_jobs_task_name', table_name='dead_letter_jobs')
    op.drop_index('idx_dead_letter_jobs_processed', table_name='dead_letter_jobs')
    op.drop_index('idx_dead_letter_jobs_moved_at', table_name='dead_letter_jobs')
    op.drop_index('idx_dead_letter_jobs_manual_intervention', table_name='dead_letter_jobs')
    op.drop_table('dead_letter_jobs')

    op.drop_index('idx_job_metrics_source', table_name='job_metrics')
    op.drop_index('idx_job_metrics_period', table_name='job_metrics')
    op.drop_index('idx_job_metrics_created_at', table_name='job_metrics')
    op.drop_table('job_metrics')

    op.drop_index('idx_scraping_jobs_task_name', table_name='scraping_jobs')
    op.drop_index('idx_scraping_jobs_status', table_name='scraping_jobs')
    op.drop_index('idx_scraping_jobs_source', table_name='scraping_jobs')
    op.drop_index('idx_scraping_jobs_scheduled_at', table_name='scraping_jobs')
    op.drop_index('idx_scraping_jobs_priority', table_name='scraping_jobs')
    op.drop_index('idx_scraping_jobs_created_at', table_name='scraping_jobs')
    op.drop_index('idx_scraping_jobs_celery_task_id', table_name='scraping_jobs')
    op.drop_table('scraping_jobs')

    # Drop ENUM types
    op.execute("DROP TYPE jobpriority")
    op.execute("DROP TYPE jobstatus")

    # ### end Alembic commands ###